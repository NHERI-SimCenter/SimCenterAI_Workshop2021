{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-written digit classification with transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will show how to build a transformer for hand-written digit classification. We will use MNIST as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9bd80fbd68>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 9999\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset using Pytorch API. First we load the training set. The training set contains 60000 images and the test set contains 10000 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST('./', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "print (len(trainset))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.MNIST('./', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "print (len(testset))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show some example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(5):\n",
    "  plt.subplot(1,5,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a transformer with Pytorch. In transformer, we have multiple encoders and decoders. Each encoder contains one self-attention layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import einsum\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super(Attention, self).__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, dim_head = 64, channels=3):\n",
    "        super(ViT, self).__init__()\n",
    "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = channels * patch_size ** 2\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        \n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        p = self.patch_size\n",
    "\n",
    "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        x = self.patch_to_embedding(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        return F.log_softmax(self.mlp_head(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT(\n",
      "  (patch_to_embedding): Linear(in_features=49, out_features=64, bias=True)\n",
      "  (transformer): Transformer(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=64, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=64, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=64, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=64, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=64, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=64, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_cls_token): Identity()\n",
      "  (mlp_head): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): GELU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
    "            dim=64, depth=6, heads=8, mlp_dim=128)\n",
    "\n",
    "print (network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the network, we need to specify the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "log_interval = 10\n",
    "\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  network.train()\n",
    "   \n",
    "  train_loss = 0\n",
    "\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = network(data)\n",
    "    \n",
    "    loss = F.nll_loss(output, target)\n",
    "    \n",
    "    train_loss += loss.item()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "  \n",
    "  train_loss /= len(train_loader.dataset)\n",
    "  train_losses.append(train_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval()\n",
    "\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  \n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.448005\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.320999\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.301966\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.259480\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.221668\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.049860\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.044194\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.918130\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.997617\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.658419\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.648923\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.624340\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.702447\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.274607\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.225619\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.238003\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.350297\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.171597\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.229294\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.876478\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.154855\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.102744\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.883254\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.893858\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.942706\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.909819\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.803414\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.689391\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.966108\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.923390\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.738267\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.493176\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.519282\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.654479\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.657002\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.444690\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.293206\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.551377\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.345652\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.463273\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.291123\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.361436\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.349510\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.376806\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.326515\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.395369\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.450577\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.338001\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.302100\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.238876\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.448355\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.455129\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.431175\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.268659\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.438483\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.278471\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.350000\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.351487\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.280262\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.266042\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.247899\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.279895\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.367915\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.157658\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.329172\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.210150\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.199544\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.306763\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.270928\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.211076\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.288513\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.156431\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.185681\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.166132\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.094939\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.234448\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.198554\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.137726\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.143388\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.303799\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.225063\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.457770\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.224610\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.150544\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.181556\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.045514\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.269334\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.258083\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.414046\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.126160\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.275171\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.154049\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.279200\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.400536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yunhui.guo/.local/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.2136, Accuracy: 9317/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.166143\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.170023\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.207539\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.124164\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.149177\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.165153\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.084363\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.070156\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.106621\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.242110\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.070600\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.314732\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.181014\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.203988\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.077841\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.210593\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.214318\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.275804\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.162735\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.243401\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.237647\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.136279\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.093007\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.091086\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.094998\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.145014\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.142461\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.288440\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.240572\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.177615\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.203620\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.165445\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.030341\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.278631\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.074980\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.229074\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.113651\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.140266\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.355938\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.109664\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.059038\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.208122\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.188972\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.222556\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.218735\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.079963\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.286255\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.134934\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.263301\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.257334\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.078390\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.174356\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.236669\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.070952\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.100287\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.074967\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.113019\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.183610\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.059451\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.236365\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.083929\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.097346\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.161608\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.387505\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.285684\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.196168\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.059258\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.163878\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.100454\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.385080\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.148994\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.047173\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.237686\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.161218\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.065769\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.255202\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.077663\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.039722\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.041167\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.225722\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.061063\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.097793\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.122822\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.234499\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.174278\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.126791\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.010274\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.042517\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.207838\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.032236\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.174299\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.171828\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.101019\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.053324\n",
      "\n",
      "Test set: Avg. loss: 0.1209, Accuracy: 9635/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.277655\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.227153\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.226311\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.017551\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.098529\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.111639\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.114625\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.143346\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.060028\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.389800\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.019215\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.150028\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.057104\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.039662\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.195862\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.175546\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.088432\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.040132\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.043429\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.102089\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.059868\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.047267\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.074391\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.073205\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.142320\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.075979\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.114788\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.070774\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.051977\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.082840\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.043624\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.139638\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.028735\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.108925\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.108136\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.089531\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.077372\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.084818\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.023868\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.075027\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.206580\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.055830\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.127732\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.153989\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.024517\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.037610\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.095827\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.075687\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.111776\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.061769\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.141228\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.022531\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.077585\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.052781\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.254286\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.228430\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.017443\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.072364\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.069788\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.067899\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.066104\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.191790\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.038090\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.075162\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.114922\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.357731\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.104056\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.202021\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.161726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.021633\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.109982\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.013154\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.068639\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.019223\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.084777\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.142689\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.036184\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.056862\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.121256\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.051808\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.045809\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.029969\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.237902\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.191801\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.069717\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.073680\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.064156\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.051883\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.060119\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.075735\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.023713\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.138721\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.046518\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.053136\n",
      "\n",
      "Test set: Avg. loss: 0.0866, Accuracy: 9740/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.138331\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.092885\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.077381\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.048620\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.051970\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.108051\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.040382\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.178971\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.022105\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.054344\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.083606\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.237083\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.107306\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.035796\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.037495\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.103487\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.038688\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.007364\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.106730\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.029407\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.097660\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.023946\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.141350\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.094258\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.059142\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.058868\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.005619\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.038423\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.073372\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.042547\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.147920\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.026991\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.066176\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.084320\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.129272\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.032986\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.008985\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.100437\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.116288\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.093893\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.044141\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.009017\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.021230\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.102161\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.123426\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.237184\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.024534\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.034391\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.131836\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.054869\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.010281\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.175651\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.085272\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.054598\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.136864\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.120988\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.042689\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.008071\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.040035\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.059742\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.021394\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.017035\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.009579\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.090056\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.012760\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.068747\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.018763\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.041412\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.065810\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.160420\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.052002\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.036050\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.355778\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.086320\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.032075\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.109265\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.039062\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.064448\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.105400\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.078023\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.104833\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.045314\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.056971\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.098185\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.031964\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.078576\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.074917\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.033381\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.034585\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.054717\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.030012\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.103794\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.016577\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.110392\n",
      "\n",
      "Test set: Avg. loss: 0.0884, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.017339\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.031662\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.043294\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.023890\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.021285\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.018327\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.051881\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.038055\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.079968\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.123342\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.117557\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.045892\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.044959\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.075990\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.025156\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.037427\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.043430\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.015241\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.039941\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.098449\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.010683\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.057624\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.110490\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.021599\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.043047\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.007589\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.003182\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.052269\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.042881\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.080077\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.097797\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.030122\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.033079\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.099566\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.087324\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.041633\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.064078\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.015968\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.007721\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.008676\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.075825\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.012145\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.110978\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.191783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.056082\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.096403\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.106642\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.028569\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.105226\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.018259\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.005007\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.222852\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.066977\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.153244\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.199600\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.012528\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.090201\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.080314\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.014165\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.175457\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.108448\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.015795\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.053316\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.006785\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.072089\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.221705\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.039800\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.134254\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.034881\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.053393\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.048501\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.089066\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.004264\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.100828\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.115138\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.097387\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.100580\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.040692\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.084159\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.081275\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.006516\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.063824\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.080661\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.079981\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.036805\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.011368\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.032203\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.082843\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.060846\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.027295\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.016610\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.085124\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.046057\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.087160\n",
      "\n",
      "Test set: Avg. loss: 0.0786, Accuracy: 9757/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.005936\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.019264\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.027843\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.014443\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.021466\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.068851\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.070058\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.037319\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.034359\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.076128\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.010749\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.055187\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.040089\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.124546\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.001506\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.002067\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.075925\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.035771\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.015499\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.010440\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.019678\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.197146\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.094790\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.063976\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.167397\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.088361\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.162186\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.019863\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.045932\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.036597\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.004423\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.044856\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.090545\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.019744\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.009576\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.032374\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.112746\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.056125\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.007701\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.032523\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.026843\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.153082\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.012954\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.020931\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.142618\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.072263\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.066530\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.015949\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.034641\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.116023\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.010243\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.093621\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.005333\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.034890\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.084585\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.129793\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.057059\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.013866\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.016239\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.067319\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.053930\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.047717\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.118352\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.035990\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.006863\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.021448\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.053118\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.118415\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.111864\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.045704\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.003601\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.028770\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.171261\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.026205\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.074656\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.046229\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.033478\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.026607\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.024436\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.035901\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.145220\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.034615\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.018730\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.037560\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.042884\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.065332\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.057067\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.064747\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.025824\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.009197\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.007342\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.176044\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.050639\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.124885\n",
      "\n",
      "Test set: Avg. loss: 0.0718, Accuracy: 9769/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.090259\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.026188\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.046421\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.048238\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.004144\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.034744\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.043568\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.161450\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.190907\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.083047\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.021626\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.012367\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.007042\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.025680\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.126804\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.006270\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.011368\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.030307\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.022636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.015867\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.015769\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.051216\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.017013\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.070271\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.098059\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.008578\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.005579\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.197007\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.040089\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.126164\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.058457\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.106497\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.066258\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.039156\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.005066\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.008589\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.022143\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.160768\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.058046\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.011383\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.001261\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.045371\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.066379\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.035250\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.026851\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.068288\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.013730\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.001668\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.006956\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.012817\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.156295\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.018181\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.005769\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.011677\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.027921\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.052310\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.020179\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.018793\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.097841\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.086822\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.018965\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.052161\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.042245\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.053082\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.076583\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.028536\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.007160\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.034813\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.088071\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.017674\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.010553\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.002672\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.056188\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.070766\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.025338\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.042901\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.095995\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.089570\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.011767\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.013195\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.006233\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.113486\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.013288\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.070786\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.012870\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.057567\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.136493\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.011033\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.025366\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.059190\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.021755\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.085129\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.099289\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.081185\n",
      "\n",
      "Test set: Avg. loss: 0.0793, Accuracy: 9753/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.060739\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.026066\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.013693\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.130686\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.130760\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.021557\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.027877\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.057046\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.042482\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.006791\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.008081\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.007041\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.056768\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.061509\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.042664\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.068124\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.007290\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.030273\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.022859\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.010174\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.004041\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.115513\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.081081\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.018745\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.026704\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.012822\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.062412\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.000825\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.019934\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.029890\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.021895\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.018201\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.013900\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.020032\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.076805\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.015811\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.026524\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.060474\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.035536\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.004247\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.063135\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.011228\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.030261\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.026921\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.005517\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.043157\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.007755\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.039668\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.053643\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.003235\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.046640\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.053568\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.006055\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.007132\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.096168\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.029673\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.035748\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.002740\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.015982\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.024369\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.032295\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.016836\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.012866\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.003423\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.067393\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.006844\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.022640\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.045288\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.059840\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.059306\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.018440\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.014174\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.044842\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.007536\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.059294\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.033432\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.009899\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.057459\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.043489\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.034141\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.026314\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.014552\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.017638\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.051035\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.043770\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.012023\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.003821\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.007551\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.005226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.001538\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.057597\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.034617\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.008270\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.002909\n",
      "\n",
      "Test set: Avg. loss: 0.0697, Accuracy: 9777/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.022430\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.007278\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.074120\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.004321\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.037631\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.032359\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.072809\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.004600\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.012672\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.016719\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.022724\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.045253\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.058900\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.038395\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.069623\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.012502\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.032841\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.006421\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.034468\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.002512\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.012490\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.002487\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.147912\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.007676\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.018502\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.013333\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.051541\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.001916\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.028584\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.008465\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.011442\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.015745\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.006548\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.004476\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.012415\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.006090\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.002060\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.006916\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.011714\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.026493\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.003630\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.053086\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.009120\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.005520\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.008041\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.010759\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.020805\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.027930\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.069736\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.045053\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.080988\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.013288\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.050777\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.003648\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.181861\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.052176\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.050258\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.005618\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.040464\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.007309\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.009222\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.033336\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.080399\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.031586\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.035239\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.000718\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.073336\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.043213\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.093350\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.118411\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.001967\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.027549\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.065947\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.011038\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.019972\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.088571\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.030666\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.039314\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.004788\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.007757\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.020953\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.020223\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.032524\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.003334\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.011344\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.030414\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.060061\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.055518\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.024584\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.003910\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.012030\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.004154\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.104448\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.076460\n",
      "\n",
      "Test set: Avg. loss: 0.0708, Accuracy: 9779/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.008557\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.009027\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.014589\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.023110\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.007555\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.031901\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.008032\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.004379\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.002128\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.007123\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.011103\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.018247\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.053309\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.023730\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.017809\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.050456\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.004804\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.092569\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.031932\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.005044\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.002327\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.029247\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.056093\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.053509\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.070646\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.029524\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.016864\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.001984\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.014395\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.111525\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.005355\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.070302\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.032262\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.023600\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.003913\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.029281\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.038520\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.004886\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.010665\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.010039\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.009190\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.000843\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.002783\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.007529\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.042881\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.022750\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.060597\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.003389\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.089782\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.031310\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.023071\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.008824\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.002067\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.003542\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.001232\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.000414\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.017579\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.006520\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.011059\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.025766\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.047599\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.010672\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.010013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.100362\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.002011\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.132875\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.004553\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.003240\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.003622\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.011890\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.082617\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.026348\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.003938\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.074347\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.036093\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.024132\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.001134\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.001710\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.005332\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.014676\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.004613\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.072011\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.057586\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.022473\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.063650\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.010142\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.019947\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.006937\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.006733\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.006677\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.090256\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.034465\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.030634\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.061344\n",
      "\n",
      "Test set: Avg. loss: 0.0714, Accuracy: 9785/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "  \n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'negative log likelihood loss')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_losses, \"-*\", color='blue')\n",
    "plt.plot(test_losses, \"-^\", color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  output = network(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(5):\n",
    "  plt.subplot(1,5,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
